{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 02 â€“ Preprocessing\n",
        "\n",
        "This notebook demonstrates the **NLP preprocessing pipeline**: text cleaning, tokenization, stopword removal, and how it feeds into TF-IDF in the next step.\n",
        "\n",
        "**Goals:**\n",
        "- Run the same cleaning as `src/preprocess.py` (lowercasing, URLs/HTML removed, non-alpha removed).\n",
        "- Show tokenization and stopword removal (English).\n",
        "- Compare raw vs cleaned text on a few samples."
      ],
      "id": "54c5bec6"
    },
    {
      "cell_type": "code",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "source": [
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "\n",
        "def find_project_root(start_dir):\n",
        "    cur = os.path.abspath(start_dir)\n",
        "    while True:\n",
        "        if os.path.isdir(os.path.join(cur, \"data\")) and os.path.isdir(os.path.join(cur, \"src\")):\n",
        "            return cur\n",
        "        parent = os.path.dirname(cur)\n",
        "        if parent == cur:\n",
        "            raise FileNotFoundError(\"Run Jupyter from inside misinformation-detection-engine.\")\n",
        "        cur = parent\n",
        "\n",
        "PROJECT_ROOT = find_project_root(os.getcwd())\n",
        "PROCESSED_PATH = os.path.join(PROJECT_ROOT, \"data\", \"processed\", \"processed_fake_news.csv\")\n",
        "RAW_PATH = os.path.join(PROJECT_ROOT, \"data\", \"raw\", \"fake_news.csv\")\n",
        "\n",
        "if os.path.exists(PROCESSED_PATH):\n",
        "    df = pd.read_csv(PROCESSED_PATH)\n",
        "    print(\"Loaded processed dataset. Columns:\", list(df.columns))\n",
        "else:\n",
        "    raise FileNotFoundError(f\"Processed data not found at {PROCESSED_PATH}. Run: python src/preprocess.py\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "4128c286"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Text cleaning (same logic as `src/preprocess.py`)\n",
        "\n",
        "- Lowercasing\n",
        "- Remove URLs and HTML tags\n",
        "- Remove non-alphabetic characters\n",
        "- Collapse multiple spaces"
      ],
      "id": "448ac1f0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def clean_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"http\\S+|www\\.\\S+\", \" \", text)\n",
        "    text = re.sub(r\"<.*?>\", \" \", text)\n",
        "    text = re.sub(r\"[^a-z\\s]\", \" \", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "# Show before/after on a few rows\n",
        "text_col = \"clean_text\" if \"clean_text\" in df.columns else \"text\"\n",
        "sample = df.head(3)\n",
        "for i, row in sample.iterrows():\n",
        "    raw = row.get(\"text\", row[text_col])\n",
        "    cleaned = clean_text(raw) if text_col == \"text\" else row[text_col]\n",
        "    print(\"RAW (first 200 chars):\", str(raw)[:200])\n",
        "    print(\"CLEAN:\", str(cleaned)[:200])\n",
        "    print(\"---\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "457ce9aa"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tokenization & stopwords\n",
        "\n",
        "TF-IDF in `src/train.py` uses `TfidfVectorizer(..., stop_words=\"english\")`. Here we show a simple tokenization + stopword removal for illustration."
      ],
      "id": "d506e510"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
        "\n",
        "tcol = \"clean_text\" if \"clean_text\" in df.columns else \"text\"\n",
        "sample_text = df[tcol].iloc[0]\n",
        "tokens = sample_text.split()\n",
        "tokens_no_stop = [t for t in tokens if t not in ENGLISH_STOP_WORDS]\n",
        "print(\"Sample text (first 150 chars):\", sample_text[:150])\n",
        "print(\"\\nToken count (all):\", len(tokens))\n",
        "print(\"Token count (no stopwords):\", len(tokens_no_stop))\n",
        "print(\"First 20 tokens (no stop):\", tokens_no_stop[:20])"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "06e37cf1"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "- Cleaning: lowercase, no URLs/HTML, letters only, collapsed spaces.\n",
        "- Tokenization: whitespace (TF-IDF does this internally).\n",
        "- Stopwords: English stopwords removed in TF-IDF. Processed text is saved in `data/processed/` for training."
      ],
      "id": "8fd07f49"
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}