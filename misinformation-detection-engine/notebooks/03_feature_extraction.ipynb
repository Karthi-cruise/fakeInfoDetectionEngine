{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 03 – Feature Extraction (TF-IDF)\n",
        "\n",
        "This notebook explores **TF-IDF vectorization**: vocabulary size, top n-grams, and how features look before training.\n",
        "\n",
        "**Goals:**\n",
        "- Fit TF-IDF on processed text (same config as `src/train.py`).\n",
        "- Inspect vocabulary and top terms by class.\n",
        "- Visualize feature importance / top keywords (basic explainability)."
      ],
      "id": "6c5e7adb"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from IPython.display import display\n",
        "\n",
        "def find_project_root(start_dir):\n",
        "    cur = os.path.abspath(start_dir)\n",
        "    while True:\n",
        "        if os.path.isdir(os.path.join(cur, \"data\")) and os.path.isdir(os.path.join(cur, \"src\")):\n",
        "            return cur\n",
        "        parent = os.path.dirname(cur)\n",
        "        if parent == cur:\n",
        "            raise FileNotFoundError(\"Run Jupyter from inside misinformation-detection-engine.\")\n",
        "        cur = parent\n",
        "\n",
        "PROJECT_ROOT = find_project_root(os.getcwd())\n",
        "PROCESSED_PATH = os.path.join(PROJECT_ROOT, \"data\", \"processed\", \"processed_fake_news.csv\")\n",
        "df = pd.read_csv(PROCESSED_PATH)\n",
        "tcol = \"clean_text\" if \"clean_text\" in df.columns else \"text\"\n",
        "X = df[tcol].astype(str)\n",
        "y = df[\"label\"].astype(int)\n",
        "print(\"Loaded processed data. Shape:\", df.shape)"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "1ff47934"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fit TF-IDF (same config as `src/train.py`)\n",
        "\n",
        "- max_features=10000, ngram_range=(1,2), stop_words=\"english\""
      ],
      "id": "9231365b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "vectorizer = TfidfVectorizer(max_features=10000, ngram_range=(1, 2), stop_words=\"english\")\n",
        "X_vec = vectorizer.fit_transform(X)\n",
        "vocab = vectorizer.get_feature_names_out()\n",
        "print(\"Vocabulary size:\", len(vocab))\n",
        "print(\"Matrix shape:\", X_vec.shape)\n",
        "print(\"Sample terms:\", list(vocab[:20]))"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "71c31e31"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Top terms by class (basic explainability)\n",
        "\n",
        "For each class we show the terms with highest average TF-IDF."
      ],
      "id": "cfda54a1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Mean TF-IDF per feature, per class\n",
        "real_mask = (y == 0)\n",
        "fake_mask = (y == 1)\n",
        "mean_real = np.asarray(X_vec[real_mask].mean(axis=0)).flatten()\n",
        "mean_fake = np.asarray(X_vec[fake_mask].mean(axis=0)).flatten()\n",
        "\n",
        "top_n = 15\n",
        "top_real_idx = np.argsort(mean_real)[-top_n:][::-1]\n",
        "top_fake_idx = np.argsort(mean_fake)[-top_n:][::-1]\n",
        "\n",
        "print(\"Top terms (avg TF-IDF) for REAL (0):\")\n",
        "for i in top_real_idx:\n",
        "    print(f\"  {vocab[i]}: {mean_real[i]:.4f}\")\n",
        "print(\"\\nTop terms (avg TF-IDF) for FAKE (1):\")\n",
        "for i in top_fake_idx:\n",
        "    print(f\"  {vocab[i]}: {mean_fake[i]:.4f}\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "f683018c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Bar plot: top 10 terms per class\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "n_show = 10\n",
        "axes[0].barh(range(n_show), mean_real[top_real_idx[:n_show]], color=\"steelblue\")\n",
        "axes[0].set_yticks(range(n_show))\n",
        "axes[0].set_yticklabels([vocab[i] for i in top_real_idx[:n_show]])\n",
        "axes[0].set_title(\"Top terms — REAL\")\n",
        "axes[0].invert_yaxis()\n",
        "axes[1].barh(range(n_show), mean_fake[top_fake_idx[:n_show]], color=\"coral\")\n",
        "axes[1].set_yticks(range(n_show))\n",
        "axes[1].set_yticklabels([vocab[i] for i in top_fake_idx[:n_show]])\n",
        "axes[1].set_title(\"Top terms — FAKE\")\n",
        "axes[1].invert_yaxis()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "ada92f0d",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}