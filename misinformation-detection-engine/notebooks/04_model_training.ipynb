{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 04 â€“ Model Training & Evaluation\n",
        "\n",
        "This notebook trains and evaluates **Naive Bayes**, **Logistic Regression**, and **Random Forest** on TF-IDF features.\n",
        "\n",
        "**Goals:**\n",
        "- Train all three models; build a **model comparison table** (accuracy, precision, recall, F1, confusion matrix).\n",
        "- **Error analysis**: sample false positives and false negatives.\n",
        "- **Confidence scoring**: e.g. \"Fake with 76% confidence\".\n",
        "- **Custom input testing**: paste your own text and get prediction + confidence + top keywords."
      ],
      "id": "5cd0ffdc"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load data and models\n",
        "\n",
        "Either run `python src/train.py` first to train and save models, or train in this notebook. Below we load saved models if present."
      ],
      "id": "8e45e15d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "import sys\n",
        "import joblib\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
        "from IPython.display import display\n",
        "\n",
        "def find_project_root(start_dir):\n",
        "    cur = os.path.abspath(start_dir)\n",
        "    while True:\n",
        "        if os.path.isdir(os.path.join(cur, \"data\")) and os.path.isdir(os.path.join(cur, \"src\")):\n",
        "            return cur\n",
        "        parent = os.path.dirname(cur)\n",
        "        if parent == cur:\n",
        "            raise FileNotFoundError(\"Run Jupyter from inside misinformation-detection-engine.\")\n",
        "        cur = parent\n",
        "\n",
        "PROJECT_ROOT = find_project_root(os.getcwd())\n",
        "PROCESSED_PATH = os.path.join(PROJECT_ROOT, \"data\", \"processed\", \"processed_fake_news.csv\")\n",
        "MODELS_DIR = os.path.join(PROJECT_ROOT, \"models\")\n",
        "\n",
        "df = pd.read_csv(PROCESSED_PATH)\n",
        "tcol = \"clean_text\" if \"clean_text\" in df.columns else \"text\"\n",
        "X = df[tcol].astype(str)\n",
        "y = df[\"label\"].astype(int)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "print(\"Data loaded. Train size:\", len(X_train), \"Val size:\", len(X_val))"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "9b85b175"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load or fit vectorizer and models\n",
        "vec_path = os.path.join(MODELS_DIR, \"tfidf_vectorizer.joblib\")\n",
        "if os.path.exists(vec_path):\n",
        "    vectorizer = joblib.load(vec_path)\n",
        "    print(\"Loaded saved TF-IDF vectorizer.\")\n",
        "else:\n",
        "    vectorizer = TfidfVectorizer(max_features=10000, ngram_range=(1, 2), stop_words=\"english\")\n",
        "    vectorizer.fit(X_train)\n",
        "    print(\"Fitted TF-IDF vectorizer (not saved). Run src/train.py to save.\")\n",
        "\n",
        "X_train_vec = vectorizer.transform(X_train)\n",
        "X_val_vec = vectorizer.transform(X_val)"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "4c204f4b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load or train models\n",
        "models = {}\n",
        "for name, path in [(\"naive_bayes\", \"naive_bayes.joblib\"), (\"log_reg\", \"log_reg.joblib\"), (\"random_forest\", \"random_forest.joblib\")]:\n",
        "    full_path = os.path.join(MODELS_DIR, path)\n",
        "    if os.path.exists(full_path):\n",
        "        models[name] = joblib.load(full_path)\n",
        "        print(f\"Loaded {name}.\")\n",
        "    else:\n",
        "        if name == \"naive_bayes\":\n",
        "            models[name] = MultinomialNB()\n",
        "        elif name == \"log_reg\":\n",
        "            models[name] = LogisticRegression(max_iter=1000, n_jobs=-1)\n",
        "        else:\n",
        "            models[name] = RandomForestClassifier(n_estimators=200, max_depth=None, n_jobs=-1, random_state=42)\n",
        "        models[name].fit(X_train_vec, y_train)\n",
        "        print(f\"Trained {name} (not saved). Run src/train.py to save.\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "e7f299e5"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model comparison table\n",
        "\n",
        "Accuracy, Precision, Recall, F1, Confusion matrix per model."
      ],
      "id": "94f5c2dc"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "rows = []\n",
        "for name, model in models.items():\n",
        "    y_pred = model.predict(X_val_vec)\n",
        "    rows.append({\n",
        "        \"model\": name,\n",
        "        \"accuracy\": accuracy_score(y_val, y_pred),\n",
        "        \"precision\": precision_score(y_val, y_pred, zero_division=0),\n",
        "        \"recall\": recall_score(y_val, y_pred, zero_division=0),\n",
        "        \"f1\": f1_score(y_val, y_pred, zero_division=0),\n",
        "    })\n",
        "    print(f\"\\n=== {name} ===\")\n",
        "    print(classification_report(y_val, y_pred, digits=4))\n",
        "    print(\"Confusion matrix:\\n\", confusion_matrix(y_val, y_pred))\n",
        "\n",
        "comparison_df = pd.DataFrame(rows)\n",
        "display(comparison_df)"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "cf4079c0"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Error analysis\n",
        "\n",
        "Sample false positives (real misclassified as fake) and false negatives (fake misclassified as real)."
      ],
      "id": "904e441c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Use log_reg for error analysis (or pick any model)\n",
        "model_name = \"log_reg\"\n",
        "y_pred = models[model_name].predict(X_val_vec)\n",
        "X_val_arr = X_val.values\n",
        "fp_idx = np.where((y_val.values == 0) & (y_pred == 1))[0][:5]\n",
        "fn_idx = np.where((y_val.values == 1) & (y_pred == 0))[0][:5]\n",
        "print(\"False positives (true=real, pred=fake):\")\n",
        "for i in fp_idx:\n",
        "    print(\"-\", X_val_arr[i][:200], \"...\")\n",
        "print(\"\\nFalse negatives (true=fake, pred=real):\")\n",
        "for i in fn_idx:\n",
        "    print(\"-\", X_val_arr[i][:200], \"...\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "7d7bdcac"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Confidence scoring & custom input\n",
        "\n",
        "Predict with confidence (e.g. \"Fake with 76% confidence\") and show top keywords."
      ],
      "id": "a1a59b9f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def predict_with_confidence(text, model_name=\"log_reg\", vectorizer=vectorizer, models=models):\n",
        "    model = models[model_name]\n",
        "    X_vec = vectorizer.transform([text])\n",
        "    pred = model.predict(X_vec)[0]\n",
        "    label_str = \"fake\" if pred == 1 else \"real\"\n",
        "    if hasattr(model, \"predict_proba\"):\n",
        "        proba = model.predict_proba(X_vec)[0]\n",
        "        conf = float(np.max(proba))\n",
        "        print(f\"Prediction: {label_str.upper()} with {conf*100:.2f}% confidence\")\n",
        "    else:\n",
        "        print(f\"Prediction: {label_str.upper()}\")\n",
        "    return pred\n",
        "\n",
        "# Example: custom input\n",
        "custom_text = \"The president announced a new policy today. Officials confirmed the details.\"\n",
        "print(\"Custom input:\", custom_text)\n",
        "predict_with_confidence(custom_text)"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "507bbaec"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Top keywords influencing prediction (Logistic Regression)\n",
        "vocab = vectorizer.get_feature_names_out()\n",
        "lr = models[\"log_reg\"]\n",
        "coef = lr.coef_[0]\n",
        "top_fake = np.argsort(coef)[-10:][::-1]\n",
        "top_real = np.argsort(coef)[:10]\n",
        "print(\"Top keywords for FAKE:\", [vocab[i] for i in top_fake])\n",
        "print(\"Top keywords for REAL:\", [vocab[i] for i in top_real])"
      ],
      "id": "661640e5",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}